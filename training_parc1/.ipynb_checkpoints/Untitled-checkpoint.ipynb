{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import QuickNAT as QN\n",
    "import torch.nn as nn\n",
    "from livelossplot import PlotLosses\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"Training dataset with mask image mapping to classes\"\"\"\n",
    "    def __init__(self, T1a_dir, parc1a_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            T1a_dir (string): Directory with T1w image in axial plane\n",
    "            transform (callable): Optional transform to be applied on a sample\n",
    "            parc1a_dir (string): Directory with parcellation scale 5 in axial plane\n",
    "        \"\"\"\n",
    "        self._T1a_dir = T1a_dir\n",
    "        self.transform = transform\n",
    "        self._parc1a_dir = parc1a_dir\n",
    "        self.mapping = {\n",
    "            180:91\n",
    "        }\n",
    "        \n",
    "    def mask_to_class(self, mask):\n",
    "        for k in self.mapping:\n",
    "            mask[mask==k] = self.mapping[k]\n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        T1a_list = os.listdir(self._T1a_dir)\n",
    "        return len(T1a_list)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        T1a_list = os.listdir(self._T1a_dir)\n",
    "        parc1a_list = os.listdir(self._parc1a_dir)\n",
    "        \n",
    "        T1a_str = T1a_list[idx]\n",
    "        \n",
    "        T1a_arr = io.imread(os.path.join(self._T1a_dir, T1a_str))\n",
    "        T1a_tensor = torch.from_numpy(T1a_arr)\n",
    "        \n",
    "        compose_T1 = transforms.Compose([transforms.ToPILImage(), \n",
    "                                         transforms.Resize((128,128),interpolation=Image.NEAREST),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        T1a_tensor = torch.unsqueeze(T1a_tensor, dim = 0)\n",
    "        T1a_tensor = compose_T1(T1a_tensor)\n",
    "              \n",
    "        parc1a_str = parc1a_list[idx]\n",
    "    \n",
    "        parc1a_arr = io.imread(os.path.join(self._parc1a_dir, parc1a_str))\n",
    "        parc1a_tensor = torch.from_numpy(parc1a_arr)\n",
    "        \n",
    "        compose = transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.Resize((128,128),interpolation=Image.NEAREST), \n",
    "                                      transforms.ToTensor()])\n",
    "        \n",
    "        parc1a_tensor = torch.unsqueeze(parc1a_tensor, dim = 0)\n",
    "        parc1a_tensor = compose(parc1a_tensor)\n",
    "        parc1a_tensor = parc1a_tensor.squeeze()\n",
    "        \n",
    "        parc1a_tensor = torch.round(parc1a_tensor / 0.0039).byte()\n",
    "        parc1a_tensor = self.mask_to_class(parc1a_tensor)\n",
    "      \n",
    "        sample = {'T1a':T1a_tensor, 'parc1a':parc1a_tensor}\n",
    "        \n",
    "        if self.transform:\n",
    "            T1a = self.transform(T1a_tensor)\n",
    "            sample = {'T1a':T1a, 'parc1a':parc1a}\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx = 0\n",
    "slice_idx = 0\n",
    "T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "total_data_0 = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    }
   ],
   "source": [
    "for sub_idx in range(1,330):\n",
    "    T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    train_data = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)\n",
    "    total_data_0 = total_data_0 + train_data\n",
    "print(len(total_data_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_0 = DataLoader(total_data_0, batch_size = 5, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "model = QN.QuickNAT(1,64,178)\n",
    "model = model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters() ,lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss:\n",
      "training   (min:    0.008, max:    0.094, cur:    0.008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xiaoyu/miniconda3/envs/deep_mol/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c5b9d6505b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Define the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep_mol/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep_mol/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "liveloss = PlotLosses()\n",
    "for epoch in range(0,50):\n",
    "    logs = {}\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_0):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get the inputs\n",
    "        inputs, labels = sample_batched['T1a'], sample_batched['parc1a']\n",
    "        \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        #forward + backward +optimize\n",
    "        scores = model(inputs)\n",
    "\n",
    "          \n",
    "        # Define the loss\n",
    "        loss = criterion(scores, labels.long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute and accumulate stats\n",
    "        running_loss += loss.detach().item()\n",
    "       \n",
    "        num_batches+=1 \n",
    "        \n",
    "        \n",
    "    # AVERAGE STATS THEN DISPLAY    \n",
    "    total_loss = running_loss/num_batches\n",
    "   \n",
    "    elapsed = (time.time()-start)/60\n",
    "        \n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t loss=', total_loss )\n",
    "    logs['log loss'] = total_loss\n",
    "            \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    " \n",
    "print('Finish Training')\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx = 0\n",
    "slice_idx = 1\n",
    "T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "total_data_1 = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_idx in range(1,330):\n",
    "    T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    train_data = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)\n",
    "    total_data_1 = total_data_1 + train_data\n",
    "print(len(total_data_1))\n",
    "dataloader_1 = DataLoader(total_data_1, batch_size = 5, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()\n",
    "for epoch in range(0,50):\n",
    "    logs = {}\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_1):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get the inputs\n",
    "        inputs, labels = sample_batched['T1a'], sample_batched['parc1a']\n",
    "        \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        #forward + backward +optimize\n",
    "        scores = model(inputs)\n",
    "\n",
    "          \n",
    "        # Define the loss\n",
    "        loss = criterion(scores, labels.long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute and accumulate stats\n",
    "        running_loss += loss.detach().item()\n",
    "       \n",
    "        num_batches+=1 \n",
    "        \n",
    "        \n",
    "    # AVERAGE STATS THEN DISPLAY    \n",
    "    total_loss = running_loss/num_batches\n",
    "   \n",
    "    elapsed = (time.time()-start)/60\n",
    "        \n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t loss=', total_loss )\n",
    "    logs['log loss'] = total_loss\n",
    "            \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    " \n",
    "print('Finish Training')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx = 0\n",
    "slice_idx = 2\n",
    "T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "total_data_1 = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)\n",
    "for sub_idx in range(1,330):\n",
    "    T1a_dir = '/home/xiaoyu/MRIdata_group/T1w/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    parc1a_dir = '/home/xiaoyu/MRIdata_group/parc_1/axial/sub{}/slice{}'.format(sub_idx,slice_idx)\n",
    "    train_data = TrainDataset(T1a_dir=T1a_dir, parc1a_dir = parc1a_dir)\n",
    "    total_data_2 = total_data_2 + train_data\n",
    "print(len(total_data_2))\n",
    "dataloader_2 = DataLoader(total_data_2, batch_size = 5, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()\n",
    "for epoch in range(0,50):\n",
    "    logs = {}\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader_2):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get the inputs\n",
    "        inputs, labels = sample_batched['T1a'], sample_batched['parc1a']\n",
    "        \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        #forward + backward +optimize\n",
    "        scores = model(inputs)\n",
    "\n",
    "          \n",
    "        # Define the loss\n",
    "        loss = criterion(scores, labels.long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute and accumulate stats\n",
    "        running_loss += loss.detach().item()\n",
    "       \n",
    "        num_batches+=1 \n",
    "        \n",
    "        \n",
    "    # AVERAGE STATS THEN DISPLAY    \n",
    "    total_loss = running_loss/num_batches\n",
    "   \n",
    "    elapsed = (time.time()-start)/60\n",
    "        \n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t loss=', total_loss )\n",
    "    logs['log loss'] = total_loss\n",
    "            \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    " \n",
    "print('Finish Training')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
