{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nn_common_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4900d980abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_common_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqueeze_and_excitation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqueeze_and_excitation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nn_common_modules'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nn_common_modules import modules as sm\n",
    "from squeeze_and_excitation import squeeze_and_excitation as se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickNat(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of QuickNAT\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        :param params: {'num_channels':1,\n",
    "                        'num_filters':64,\n",
    "                        'kernel_h':5,\n",
    "                        'kernel_w':5,\n",
    "                        'stride_conv':1,\n",
    "                        'pool':2,\n",
    "                        'stride_pool':2,\n",
    "                        'num_classes':28\n",
    "                        'se_block': False,\n",
    "                        'drop_out':0.2}\n",
    "        \"\"\"\n",
    "        super(QuickNat, self).__init__()\n",
    "\n",
    "        self.encode1 = sm.EncoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        params['num_channels'] = 64\n",
    "        self.encode2 = sm.EncoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.encode3 = sm.EncoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.encode4 = sm.EncoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.bottleneck = sm.DenseBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        params['num_channels'] = 128\n",
    "        self.decode1 = sm.DecoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.decode2 = sm.DecoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.decode3 = sm.DecoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        self.decode4 = sm.DecoderBlock(params, se_block_type=se.SELayer.SSE)\n",
    "        params['num_channels'] = 64\n",
    "        self.classifier = sm.ClassifierBlock(params)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param input: X\n",
    "        :return: probabiliy map\n",
    "        \"\"\"\n",
    "        e1, out1, ind1 = self.encode1.forward(input)\n",
    "        e2, out2, ind2 = self.encode2.forward(e1)\n",
    "        e3, out3, ind3 = self.encode3.forward(e2)\n",
    "        e4, out4, ind4 = self.encode4.forward(e3)\n",
    "\n",
    "        bn = self.bottleneck.forward(e4)\n",
    "\n",
    "        d4 = self.decode4.forward(bn, out4, ind4)\n",
    "        d3 = self.decode1.forward(d4, out3, ind3)\n",
    "        d2 = self.decode2.forward(d3, out2, ind2)\n",
    "        d1 = self.decode3.forward(d2, out1, ind1)\n",
    "        prob = self.classifier.forward(d1)\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def enable_test_dropout(self):\n",
    "        \"\"\"\n",
    "        Enables test time drop out for uncertainity\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        attr_dict = self.__dict__['_modules']\n",
    "        for i in range(1, 5):\n",
    "            encode_block, decode_block = attr_dict['encode' + str(i)], attr_dict['decode' + str(i)]\n",
    "            encode_block.drop_out = encode_block.drop_out.apply(nn.Module.train)\n",
    "            decode_block.drop_out = decode_block.drop_out.apply(nn.Module.train)\n",
    "\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        \"\"\"\n",
    "        Check if model parameters are allocated on the GPU.\n",
    "        \"\"\"\n",
    "        return next(self.parameters()).is_cuda\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save model with its parameters to the given path. Conventionally the\n",
    "        path should end with '*.model'.\n",
    "        Inputs:\n",
    "        - path: path string\n",
    "        \"\"\"\n",
    "        print('Saving model... %s' % path)\n",
    "        torch.save(self, path)\n",
    "\n",
    "    def predict(self, X, device=0, enable_dropout=False):\n",
    "        \"\"\"\n",
    "        Predicts the outout after the model is trained.\n",
    "        Inputs:\n",
    "        - X: Volume to be predicted\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        if type(X) is np.ndarray:\n",
    "            X = torch.tensor(X, requires_grad=False).type(torch.FloatTensor).cuda(device, non_blocking=True)\n",
    "        elif type(X) is torch.Tensor and not X.is_cuda:\n",
    "            X = X.type(torch.FloatTensor).cuda(device, non_blocking=True)\n",
    "\n",
    "        if enable_dropout:\n",
    "            self.enable_test_dropout()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(X)\n",
    "\n",
    "        max_val, idx = torch.max(out, 1)\n",
    "        idx = idx.data.cpu().numpy()\n",
    "        prediction = np.squeeze(idx)\n",
    "        del X, out, idx, max_val\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
