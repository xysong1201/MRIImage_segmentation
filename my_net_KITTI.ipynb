{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to implement a simple convnet using the same datasets I used for UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import utils_xy\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 1\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data observation\n",
    "* we have 160 training images and 160 training mask (groundtruth label for every pixel in the training images).\n",
    "* for training images, the data type is uint8\n",
    "* for training mask, the data type is uint8\n",
    "* the image size is near 375x1242x3, some maybe 370x1242x3\n",
    "* the mask size is near 375z1242x3, some maybe 370x1242x3\n",
    "\n",
    "since the image size are different in the training set, we need to do the data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images numbers for different folder of training images: 160\n",
      "Training mask Images numbers:160\n"
     ]
    }
   ],
   "source": [
    "# load the training images and the semantic segmentation image (mask image)\n",
    "img_dir = '/home/xiaoyu/data_semantics/training/train/image/'\n",
    "mask_dir = '/home/xiaoyu/data_semantics/training/train/mask_rgb/'\n",
    "\n",
    "img_list = os.listdir(img_dir)\n",
    "mask_list = os.listdir(mask_dir)\n",
    "\n",
    "print(\"Training images numbers for different folder of training images: \"+str(len(img_list)))\n",
    "print(\"Training mask Images numbers:\"+str(len(mask_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the class mapping of the mask. There are 29 classes in the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 3])\n",
      "tensor([[  0.,   0.,   0.],\n",
      "        [  0.,   0.,  70.],\n",
      "        [  0.,   0.,  90.],\n",
      "        [  0.,   0., 110.],\n",
      "        [  0.,   0., 142.],\n",
      "        [  0.,   0., 230.],\n",
      "        [  0.,  60., 100.],\n",
      "        [  0.,  80., 100.],\n",
      "        [ 70.,  70.,  70.],\n",
      "        [ 70., 130., 180.],\n",
      "        [ 81.,   0.,  81.],\n",
      "        [102., 102., 156.],\n",
      "        [107., 142.,  35.],\n",
      "        [111.,  74.,   0.],\n",
      "        [119.,  11.,  32.],\n",
      "        [128.,  64., 128.],\n",
      "        [150., 100., 100.],\n",
      "        [150., 120.,  90.],\n",
      "        [152., 251., 152.],\n",
      "        [153., 153., 153.],\n",
      "        [180., 165., 180.],\n",
      "        [190., 153., 153.],\n",
      "        [220.,  20.,  60.],\n",
      "        [220., 220.,   0.],\n",
      "        [230., 150., 140.],\n",
      "        [244.,  35., 232.],\n",
      "        [250., 170.,  30.],\n",
      "        [250., 170., 160.],\n",
      "        [255.,   0.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "colors_all = torch.tensor([])\n",
    "for i in range(len(mask_list)):\n",
    "    mask_str = mask_list[i]\n",
    "    mask_arr = io.imread(os.path.join(mask_dir, mask_str))\n",
    "    mask_tensor = torch.from_numpy(mask_arr)\n",
    "    mask_tensor = mask_tensor.permute(2,0,1)\n",
    "    \n",
    "#     print(mask_tensor.shape)\n",
    "    colors = torch.unique(mask_tensor.view(mask_tensor.size(0), -1),dim=1)\n",
    "    colors = colors.permute(1,0).type(torch.FloatTensor) \n",
    "    colors_all = torch.cat((colors_all, colors))\n",
    "    colors_unique = torch.unique(colors_all, dim = 0)\n",
    "print(colors_unique.shape)\n",
    "print(colors_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Preprocess of the data\n",
    "* construction of the dataset class \n",
    "* normalisation of the training image (mean subtraction of the training image before feed into the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "      \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.mapping = {\n",
    "        torch.tensor([  0,   0,   0], dtype=torch.uint8):0,\n",
    "        torch.tensor([  0,   0,  70], dtype=torch.uint8):1,\n",
    "        torch.tensor([  0,   0,  90], dtype=torch.uint8):2,\n",
    "        torch.tensor([  0,   0, 110], dtype=torch.uint8):3,\n",
    "        torch.tensor([  0,   0, 142], dtype=torch.uint8):4,\n",
    "        torch.tensor([  0,   0, 230], dtype=torch.uint8):5,\n",
    "        torch.tensor([  0,  60, 100], dtype=torch.uint8):6,\n",
    "        torch.tensor([  0,  80, 100.], dtype=torch.uint8):7,\n",
    "        torch.tensor([ 70,  70,  70], dtype=torch.uint8):8,\n",
    "        torch.tensor([ 70, 130, 180], dtype=torch.uint8):9,\n",
    "        torch.tensor([ 81,   0,  81], dtype=torch.uint8):10,\n",
    "        torch.tensor([102, 102, 156], dtype=torch.uint8):11,\n",
    "        torch.tensor([107, 142,  35], dtype=torch.uint8):12,\n",
    "        torch.tensor([111,  74,   0], dtype=torch.uint8):13,\n",
    "        torch.tensor([119,  11,  32], dtype=torch.uint8):14,\n",
    "        torch.tensor([128,  64, 128], dtype=torch.uint8):15,\n",
    "        torch.tensor([150, 100, 100], dtype=torch.uint8):16,\n",
    "        torch.tensor([150, 120,  90], dtype=torch.uint8):17,\n",
    "        torch.tensor([152, 251, 152], dtype=torch.uint8):18,\n",
    "        torch.tensor([153, 153, 153], dtype=torch.uint8):19,\n",
    "        torch.tensor([180, 165, 180], dtype=torch.uint8):20,\n",
    "        torch.tensor([190, 153, 153], dtype=torch.uint8):21,\n",
    "        torch.tensor([220,  20,  60], dtype=torch.uint8):22,\n",
    "        torch.tensor([220, 220,   0], dtype=torch.uint8):23,\n",
    "        torch.tensor([230, 150, 140], dtype=torch.uint8):24,\n",
    "        torch.tensor([244,  35, 232], dtype=torch.uint8):25,\n",
    "        torch.tensor([250, 170,  30], dtype=torch.uint8):26,\n",
    "        torch.tensor([250, 170, 160], dtype=torch.uint8):27,\n",
    "        torch.tensor([255,   0,   0], dtype=torch.uint8):28\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "    \n",
    "    def mask_to_class(self, mask):\n",
    "        for k in self.mapping:\n",
    "#             print(k.dtype)\n",
    "#             print(mask.dtype)\n",
    "            mask[mask==k] = self.mapping[k]\n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_list = os.listdir(img_dir)\n",
    "        mask_list = os.listdir(mask_dir)\n",
    "        \n",
    "        img_str = img_list[idx]\n",
    "        img_arr = io.imread(os.path.join(img_dir, img_str))\n",
    "        img_tensor = torch.from_numpy(img_arr)\n",
    "        img_tensor = img_tensor.permute(2,0,1)\n",
    "        \n",
    "        mask_str = mask_list[idx]\n",
    "        mask_arr = io.imread(os.path.join(mask_dir, mask_str))\n",
    "        mask_tensor = torch.from_numpy(mask_arr)\n",
    "        mask_tensor = mask_tensor.permute(2,0,1)\n",
    "        mask_tensor = mask_tensor.view(mask_tensor.size(0), -1).permute(1,0)\n",
    "        print(mask_tensor)\n",
    "        print(mask_tensor.shape)\n",
    "  \n",
    "        mask_tensor = self.mask_to_class(mask_tensor)\n",
    "        print(mask_tensor.shape)\n",
    "        sample = {'image':img_tensor, 'mask':mask_tensor}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 70, 130, 180],\n",
      "        [ 70, 130, 180],\n",
      "        [ 70, 130, 180],\n",
      "        ...,\n",
      "        [244,  35, 232],\n",
      "        [244,  35, 232],\n",
      "        [244,  35, 232]], dtype=torch.uint8)\n",
      "torch.Size([465750, 3])\n",
      "torch.Size([465750, 3])\n",
      "tensor([[ 8,  9,  9],\n",
      "        [ 8,  9,  9],\n",
      "        [ 8,  9,  9],\n",
      "        ...,\n",
      "        [25, 25, 25],\n",
      "        [25, 25, 25],\n",
      "        [25, 25, 25]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "traindata = TrainDataset(img_dir = img_dir, mask_dir = mask_dir)\n",
    "print(traindata[0]['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMoreMath.cpp:1324",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-0de98039c23a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMoreMath.cpp:1324"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[7,8,9]])\n",
    "print(a.shape)\n",
    "b = torch.tensor([4,5,6])\n",
    "c =torch.stack((a,b),dim=0)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0,0],[0,90],[111,0],[119,32]])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "       torch.tensor([0,0]):0,\n",
    "       torch.tensor([0,90]):1,\n",
    "       torch.tensor([111,0]):2,\n",
    "       torch.tensor([119,32]):3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=torch.uint8)\n",
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=torch.uint8)\n",
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=torch.uint8)\n",
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "for i in mapping:\n",
    "    print(a==i)\n",
    "    a[a==i] = mapping[i]\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
