{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to implement a simple convnet using the same datasets I used for UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import utils_xy\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 1\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data observation\n",
    "* we have 160 training images and 160 training mask (groundtruth label for every pixel in the training images).\n",
    "* for training images, the data type is uint8\n",
    "* for training mask, the data type is uint8\n",
    "* the image size is near 375x1242x3, some maybe 370x1242x3\n",
    "* the mask size is near 375z1242x3, some maybe 370x1242x3\n",
    "\n",
    "since the image size are different in the training set, we need to do the data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images numbers for different folder of training images: 160\n",
      "Training mask Images numbers:160\n"
     ]
    }
   ],
   "source": [
    "# load the training images and the semantic segmentation image (mask image)\n",
    "img_dir = '/home/xiaoyu/data_semantics/training/train/image/'\n",
    "mask_dir = '/home/xiaoyu/data_semantics/training/train/mask_rgb/'\n",
    "\n",
    "img_list = os.listdir(img_dir)\n",
    "mask_list = os.listdir(mask_dir)\n",
    "\n",
    "print(\"Training images numbers for different folder of training images: \"+str(len(img_list)))\n",
    "print(\"Training mask Images numbers:\"+str(len(mask_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the class mapping of the mask. i.e. Get the classes of the images. There are 29 classes in total. For each pixel in the image, there should be a label assigned to this pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 3])\n",
      "tensor([[  0.,   0.,   0.],\n",
      "        [  0.,   0.,  70.],\n",
      "        [  0.,   0.,  90.],\n",
      "        [  0.,   0., 110.],\n",
      "        [  0.,   0., 142.],\n",
      "        [  0.,   0., 230.],\n",
      "        [  0.,  60., 100.],\n",
      "        [  0.,  80., 100.],\n",
      "        [ 70.,  70.,  70.],\n",
      "        [ 70., 130., 180.],\n",
      "        [ 81.,   0.,  81.],\n",
      "        [102., 102., 156.],\n",
      "        [107., 142.,  35.],\n",
      "        [111.,  74.,   0.],\n",
      "        [119.,  11.,  32.],\n",
      "        [128.,  64., 128.],\n",
      "        [150., 100., 100.],\n",
      "        [150., 120.,  90.],\n",
      "        [152., 251., 152.],\n",
      "        [153., 153., 153.],\n",
      "        [180., 165., 180.],\n",
      "        [190., 153., 153.],\n",
      "        [220.,  20.,  60.],\n",
      "        [220., 220.,   0.],\n",
      "        [230., 150., 140.],\n",
      "        [244.,  35., 232.],\n",
      "        [250., 170.,  30.],\n",
      "        [250., 170., 160.],\n",
      "        [255.,   0.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "colors_all = torch.tensor([])\n",
    "for i in range(len(mask_list)):\n",
    "    mask_str = mask_list[i]\n",
    "    mask_arr = io.imread(os.path.join(mask_dir, mask_str))\n",
    "    mask_tensor = torch.from_numpy(mask_arr)\n",
    "    mask_tensor = mask_tensor.permute(2,0,1)\n",
    "    \n",
    "    mask_tensor = mask_tensor.to(device)\n",
    "    colors = torch.unique(mask_tensor.view(mask_tensor.size(0), -1),dim=1)\n",
    "    colors = colors.permute(1,0).type(torch.FloatTensor) \n",
    "    colors_all = torch.cat((colors_all, colors))\n",
    "    colors_unique = torch.unique(colors_all, dim = 0)\n",
    "print(colors_unique.shape)\n",
    "print(colors_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Preprocess of the data\n",
    "* construction of the dataset class \n",
    "* normalisation of the training image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.I Define a Train Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "      \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.mapping = {\n",
    "        torch.tensor([  0,   0,   0], dtype=torch.uint8):0,\n",
    "        torch.tensor([  0,   0,  70], dtype=torch.uint8):1,\n",
    "        torch.tensor([  0,   0,  90], dtype=torch.uint8):2,\n",
    "        torch.tensor([  0,   0, 110], dtype=torch.uint8):3,\n",
    "        torch.tensor([  0,   0, 142], dtype=torch.uint8):4,\n",
    "        torch.tensor([  0,   0, 230], dtype=torch.uint8):5,\n",
    "        torch.tensor([  0,  60, 100], dtype=torch.uint8):6,\n",
    "        torch.tensor([  0,  80, 100.], dtype=torch.uint8):7,\n",
    "        torch.tensor([ 70,  70,  70], dtype=torch.uint8):8,\n",
    "        torch.tensor([ 70, 130, 180], dtype=torch.uint8):9,\n",
    "        torch.tensor([ 81,   0,  81], dtype=torch.uint8):10,\n",
    "        torch.tensor([102, 102, 156], dtype=torch.uint8):11,\n",
    "        torch.tensor([107, 142,  35], dtype=torch.uint8):12,\n",
    "        torch.tensor([111,  74,   0], dtype=torch.uint8):13,\n",
    "        torch.tensor([119,  11,  32], dtype=torch.uint8):14,\n",
    "        torch.tensor([128,  64, 128], dtype=torch.uint8):15,\n",
    "        torch.tensor([150, 100, 100], dtype=torch.uint8):16,\n",
    "        torch.tensor([150, 120,  90], dtype=torch.uint8):17,\n",
    "        torch.tensor([152, 251, 152], dtype=torch.uint8):18,\n",
    "        torch.tensor([153, 153, 153], dtype=torch.uint8):19,\n",
    "        torch.tensor([180, 165, 180], dtype=torch.uint8):20,\n",
    "        torch.tensor([190, 153, 153], dtype=torch.uint8):21,\n",
    "        torch.tensor([220,  20,  60], dtype=torch.uint8):22,\n",
    "        torch.tensor([220, 220,   0], dtype=torch.uint8):23,\n",
    "        torch.tensor([230, 150, 140], dtype=torch.uint8):24,\n",
    "        torch.tensor([244,  35, 232], dtype=torch.uint8):25,\n",
    "        torch.tensor([250, 170,  30], dtype=torch.uint8):26,\n",
    "        torch.tensor([250, 170, 160], dtype=torch.uint8):27,\n",
    "        torch.tensor([255,   0,   0], dtype=torch.uint8):28\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "    \n",
    "    def mask_to_class(self, mask):\n",
    "        for k in self.mapping:\n",
    "            print(k.dtype)\n",
    "            print(mask.dtype)\n",
    "#             mask[mask==k] = self.mapping[k]\n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_list = os.listdir(img_dir)\n",
    "        mask_list = os.listdir(mask_dir)\n",
    "        \n",
    "        img_str = img_list[idx]\n",
    "        img_arr = io.imread(os.path.join(img_dir, img_str))\n",
    "        img_tensor = torch.from_numpy(img_arr)\n",
    "        img_tensor = img_tensor.permute(2,0,1)\n",
    "        \n",
    "        mask_str = mask_list[idx]\n",
    "        mask_arr = io.imread(os.path.join(mask_dir, mask_str))\n",
    "        mask_tensor = torch.from_numpy(mask_arr)\n",
    "        mask_tensor = mask_tensor.permute(2,0,1)\n",
    "#         mask_tensor = mask_tensor.view(mask_tensor.size(0), -1).permute(1,0)\n",
    "        print(mask_tensor)\n",
    "        print(mask_tensor.shape)\n",
    "  \n",
    "        mask_tensor = self.mask_to_class(mask_tensor)\n",
    "    \n",
    "        print(mask_tensor.shape)\n",
    "        sample = {'image':img_tensor, 'mask':mask_tensor}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         [ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         [ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         ...,\n",
      "         [128, 128, 128,  ..., 244, 244, 244],\n",
      "         [128, 128, 128,  ..., 244, 244, 244],\n",
      "         [128, 128, 128,  ..., 244, 244, 244]],\n",
      "\n",
      "        [[130, 130, 130,  ..., 130, 130, 130],\n",
      "         [130, 130, 130,  ..., 130, 130, 130],\n",
      "         [130, 130, 130,  ..., 130, 130, 130],\n",
      "         ...,\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35],\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35],\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35]],\n",
      "\n",
      "        [[180, 180, 180,  ..., 180, 180, 180],\n",
      "         [180, 180, 180,  ..., 180, 180, 180],\n",
      "         [180, 180, 180,  ..., 180, 180, 180],\n",
      "         ...,\n",
      "         [128, 128, 128,  ..., 232, 232, 232],\n",
      "         [128, 128, 128,  ..., 232, 232, 232],\n",
      "         [128, 128, 128,  ..., 232, 232, 232]]], dtype=torch.uint8)\n",
      "torch.Size([3, 375, 1242])\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.uint8\n",
      "torch.Size([3, 375, 1242])\n",
      "tensor([[[ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         [ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         [ 70,  70,  70,  ...,  70,  70,  70],\n",
      "         ...,\n",
      "         [128, 128, 128,  ..., 244, 244, 244],\n",
      "         [128, 128, 128,  ..., 244, 244, 244],\n",
      "         [128, 128, 128,  ..., 244, 244, 244]],\n",
      "\n",
      "        [[130, 130, 130,  ..., 130, 130, 130],\n",
      "         [130, 130, 130,  ..., 130, 130, 130],\n",
      "         [130, 130, 130,  ..., 130, 130, 130],\n",
      "         ...,\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35],\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35],\n",
      "         [ 64,  64,  64,  ...,  35,  35,  35]],\n",
      "\n",
      "        [[180, 180, 180,  ..., 180, 180, 180],\n",
      "         [180, 180, 180,  ..., 180, 180, 180],\n",
      "         [180, 180, 180,  ..., 180, 180, 180],\n",
      "         ...,\n",
      "         [128, 128, 128,  ..., 232, 232, 232],\n",
      "         [128, 128, 128,  ..., 232, 232, 232],\n",
      "         [128, 128, 128,  ..., 232, 232, 232]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "traindata = TrainDataset(img_dir = img_dir, mask_dir = mask_dir)\n",
    "print(traindata[0]['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "        torch.tensor([  0,   0,   0], dtype=torch.uint8):torch.tensor([0,0,0]),\n",
    "        torch.tensor([  0,   0,  70], dtype=torch.uint8):torch.tensor([1,1,1]),\n",
    "        torch.tensor([  0,   0,  90], dtype=torch.uint8):torch.tensor([2,2,2]),\n",
    "        torch.tensor([  0,   0, 110], dtype=torch.uint8):torch.tensor([3,3,3]),\n",
    "        torch.tensor([  0,   0, 142], dtype=torch.uint8):torch.tensor([4,4,4]),\n",
    "        torch.tensor([  0,   0, 230], dtype=torch.uint8):torch.tensor([5,5,5]),\n",
    "        torch.tensor([  0,  60, 100], dtype=torch.uint8):torch.tensor([6,6,6]),\n",
    "        torch.tensor([  0,  80, 100.], dtype=torch.uint8):torch.tensor([7,7,7]),\n",
    "        torch.tensor([ 70,  70,  70], dtype=torch.uint8):torch.tensor([8,8,8]),\n",
    "        torch.tensor([ 70, 130, 180], dtype=torch.uint8):torch.tensor([9,9,9]),\n",
    "        torch.tensor([ 81,   0,  81], dtype=torch.uint8):torch.tensor([10,10,10]),\n",
    "        torch.tensor([102, 102, 156], dtype=torch.uint8):torch.tensor([11,11,11]),\n",
    "        torch.tensor([107, 142,  35], dtype=torch.uint8):torch.tensor([12,12,12]),\n",
    "        torch.tensor([111,  74,   0], dtype=torch.uint8):torch.tensor([13,13,13]),\n",
    "        torch.tensor([119,  11,  32], dtype=torch.uint8):torch.tensor([14,14,14]),\n",
    "        torch.tensor([128,  64, 128], dtype=torch.uint8):torch.tensor([15,15,15]),\n",
    "        torch.tensor([150, 100, 100], dtype=torch.uint8):torch.tensor([16,16,16]),\n",
    "        torch.tensor([150, 120,  90], dtype=torch.uint8):torch.tensor([17,17,17]),\n",
    "        torch.tensor([152, 251, 152], dtype=torch.uint8):torch.tensor([18,18,18]),\n",
    "        torch.tensor([153, 153, 153], dtype=torch.uint8):torch.tensor([19,19,19]),\n",
    "        torch.tensor([180, 165, 180], dtype=torch.uint8):torch.tensor([20,20,20]),\n",
    "        torch.tensor([190, 153, 153], dtype=torch.uint8):torch.tensor([21,21,21]),\n",
    "        torch.tensor([220,  20,  60], dtype=torch.uint8):torch.tensor([22,22,22]),\n",
    "        torch.tensor([220, 220,   0], dtype=torch.uint8):torch.tensor([23,23,23]),\n",
    "        torch.tensor([230, 150, 140], dtype=torch.uint8):torch.tensor([24,24,24]),\n",
    "        torch.tensor([244,  35, 232], dtype=torch.uint8):torch.tensor([25,25,25]),\n",
    "        torch.tensor([250, 170,  30], dtype=torch.uint8):torch.tensor([26,26,26]),\n",
    "        torch.tensor([250, 170, 160], dtype=torch.uint8):torch.tensor([27,27,27]),\n",
    "        torch.tensor([255,   0,   0], dtype=torch.uint8):torch.tensor([28,28,28])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([375, 1242, 3])\n",
      "tensor([ 70, 130, 180], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "mask_ex = mask_list[0]\n",
    "mask_arr = io.imread(os.path.join(mask_dir, mask_ex))\n",
    "mask_tensor = torch.from_numpy(mask_arr)\n",
    "print(mask_tensor.size())\n",
    "print(mask_tensor[0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0], dtype=torch.uint8)\n",
      "tensor([ 0,  0, 70], dtype=torch.uint8)\n",
      "tensor([ 0,  0, 90], dtype=torch.uint8)\n",
      "tensor([  0,   0, 110], dtype=torch.uint8)\n",
      "tensor([  0,   0, 142], dtype=torch.uint8)\n",
      "tensor([  0,   0, 230], dtype=torch.uint8)\n",
      "tensor([  0,  60, 100], dtype=torch.uint8)\n",
      "tensor([  0,  80, 100], dtype=torch.uint8)\n",
      "tensor([70, 70, 70], dtype=torch.uint8)\n",
      "tensor([ 70, 130, 180], dtype=torch.uint8)\n",
      "tensor([81,  0, 81], dtype=torch.uint8)\n",
      "tensor([102, 102, 156], dtype=torch.uint8)\n",
      "tensor([107, 142,  35], dtype=torch.uint8)\n",
      "tensor([111,  74,   0], dtype=torch.uint8)\n",
      "tensor([119,  11,  32], dtype=torch.uint8)\n",
      "tensor([128,  64, 128], dtype=torch.uint8)\n",
      "tensor([150, 100, 100], dtype=torch.uint8)\n",
      "tensor([150, 120,  90], dtype=torch.uint8)\n",
      "tensor([152, 251, 152], dtype=torch.uint8)\n",
      "tensor([153, 153, 153], dtype=torch.uint8)\n",
      "tensor([180, 165, 180], dtype=torch.uint8)\n",
      "tensor([190, 153, 153], dtype=torch.uint8)\n",
      "tensor([220,  20,  60], dtype=torch.uint8)\n",
      "tensor([220, 220,   0], dtype=torch.uint8)\n",
      "tensor([230, 150, 140], dtype=torch.uint8)\n",
      "tensor([244,  35, 232], dtype=torch.uint8)\n",
      "tensor([250, 170,  30], dtype=torch.uint8)\n",
      "tensor([250, 170, 160], dtype=torch.uint8)\n",
      "tensor([255,   0,   0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "for k in mapping:\n",
    "    print(k)\n",
    "    if any([(k == mask_tensor_).all() for mask_tensor_ in mask_tensor]):\n",
    "        print('a in c')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
