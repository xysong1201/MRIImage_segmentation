{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSELayer(nn.Module):\n",
    "    \"\"\"\n",
    "     Squeeze-and-Excitation (SE) block \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, reduction_ratio=2):\n",
    "        \"\"\"\n",
    "        :param num_channels: No of input channels\n",
    "        :param reduction_ratio: By how much should the num_channels should be reduced\n",
    "        \"\"\"\n",
    "        super(ChannelSELayer, self).__init__()\n",
    "        num_channels_reduced = num_channels // reduction_ratio\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
    "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        :param input_tensor: X, shape = (batch_size, num_channels, H, W)\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, H, W = input_tensor.size()\n",
    "        # Average along each channel\n",
    "        squeeze_tensor = input_tensor.view(batch_size, num_channels, -1).mean(dim=2)\n",
    "\n",
    "        # channel excitation\n",
    "        fc_out_1 = self.relu(self.fc1(squeeze_tensor))\n",
    "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
    "\n",
    "        a, b = squeeze_tensor.size()\n",
    "        output_tensor = torch.mul(input_tensor, fc_out_2.view(a, b, 1, 1))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialSELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Re-implementation of SE block -- squeezing spatially and exciting channel-wise described in:\n",
    "        *Roy et al., Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks, MICCAI 2018*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        \"\"\"\n",
    "        :param num_channels: No of input channels\n",
    "        \"\"\"\n",
    "        super(SpatialSELayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(num_channels, 1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor, weights=None):\n",
    "        \"\"\"\n",
    "        :param weights: weights for few shot learning\n",
    "        :param input_tensor: X, shape = (batch_size, num_channels, H, W)\n",
    "        :return: output_tensor\n",
    "        \"\"\"\n",
    "        # spatial squeeze\n",
    "        batch_size, channel, a, b = input_tensor.size()\n",
    "\n",
    "        if weights:\n",
    "            weights = weights.view(1, channel, 1, 1)\n",
    "            out = F.conv2d(input_tensor, weights)\n",
    "        else:\n",
    "            out = self.conv(input_tensor)\n",
    "        squeeze_tensor = self.sigmoid(out)\n",
    "\n",
    "        # spatial excitation\n",
    "        output_tensor = torch.mul(input_tensor, squeeze_tensor.view(batch_size, 1, a, b))\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSpatialSELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Re-implementation of concurrent spatial and channel squeeze & excitation:\n",
    "        *Roy et al., Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks, arXiv:1803.02579*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, reduction_ratio=2):\n",
    "        \"\"\"\n",
    "        :param num_channels: No of input channels\n",
    "        :param reduction_ratio: By how much should the num_channels should be reduced\n",
    "        \"\"\"\n",
    "        super(ChannelSpatialSELayer, self).__init__()\n",
    "        self.cSE = ChannelSELayer(num_channels, reduction_ratio)\n",
    "        self.sSE = SpatialSELayer(num_channels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        :param input_tensor: X, shape = (batch_size, num_channels, H, W)\n",
    "        :return: output_tensor\n",
    "        \"\"\"\n",
    "        output_tensor = torch.max(self.cSE(input_tensor), self.sSE(input_tensor))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(Enum):\n",
    "    \"\"\"\n",
    "    Enum restricting the type of SE Blockes available. So that type checking can be adding when adding these blockes to\n",
    "    a neural network::\n",
    "        if self.se_block_type == se.SELayer.CSE.value:\n",
    "            self.SELayer = se.ChannelSpatialSELayer(params['num_filters'])\n",
    "        elif self.se_block_type == se.SELayer.SSE.value:\n",
    "            self.SELayer = se.SpatialSELayer(params['num_filters'])\n",
    "        elif self.se_block_type == se.SELayer.CSSE.value:\n",
    "            self.SELayer = se.ChannelSpatialSELayer(params['num_filters'])\n",
    "    \"\"\"\n",
    "    NONE = 'NONE'\n",
    "    CSE = 'CSE'\n",
    "    SSE = 'SSE'\n",
    "    CSSE = 'CSSE'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
